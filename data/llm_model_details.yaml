# unfortunately, this likely needs to be provider dependent, e.g. azure/openai will have different rate limits on same model
# costs are per 1000 tokens
# OPENAI & AZURE
# openai docs: https://platform.openai.com/docs/models/overview
# rate limits: https://platform.openai.com/docs/guides/rate-limits?context=tier-free
gpt-3.5-turbo:
  max_tokens: 16385
  max_gen_tokens: 4096
  prompt_cost: 0.0005
  completion_cost: 0.0015
  rpm: 3500
  tpm: 90000
gpt-35-turbo:
  max_tokens: 16385
  max_gen_tokens: 4096
  prompt_cost: 0.0005
  completion_cost: 0.0015
  rpm: 3500
  tpm: 90000
gpt-3.5-turbo-16k:
  max_tokens: 16384
  prompt_cost: 0.003
  completion_cost: 0.004
  rpm: 3500
  tpm: 180000
gpt-35-turbo-0125:
  max_tokens: 16385
  max_gen_tokens: 4096
  prompt_cost: 0.0005
  completion_cost: 0.0015
  rpm: 3500
  tpm: 90000
gpt-4-turbo:
  max_tokens: 128000
  max_gen_tokens: 4096
  prompt_cost: 0.01
  completion_cost: 0.03
  rpm: 200
  tpm: 40000
gpt-4-turbo-2024-04-09:
  max_tokens: 128000
  max_gen_tokens: 4096
  prompt_cost: 0.01
  completion_cost: 0.03
  rpm: 200
  tpm: 40000
gpt-4:
  max_tokens: 8192
  prompt_cost: 0.03
  completion_cost: 0.06
  rpm: 200
  tpm: 40000
gpt-4-32k:
  max_tokens: 32768
  prompt_cost: 0.06
  completion_cost: 0.12
  rpm: 1000
  tpm: 150000
gpt-4-1106-preview:
  max_tokens: 128000
  prompt_cost: 0.01
  completion_cost: 0.3
  rpm: 1000
  tpm: 150000
# ANTHROPIC
# https://docs.anthropic.com/claude/docs/models-overview
# https://docs.anthropic.com/claude/reference/rate-limits
claude-2:
  max_tokens: 100000
  max_gen_tokens: 4096
  prompt_cost: 0.01102
  completion_cost: 0.03268
  rpm: 1
  tpm: 100000
claude-2.0: # https://docs.anthropic.com/claude/reference/errors-and-rate-limits
  max_tokens: 100000
  max_gen_tokens: 4096
  prompt_cost: 0.01102
  completion_cost: 0.03268
  rpm: 1
  tpm: 100000
claude-2.1: # https://docs.anthropic.com/claude/reference/input-and-output-sizes
  max_tokens: 200000
  max_gen_tokens: 4096
  prompt_cost: 0.01102
  completion_cost: 0.03268
  rpm: 1
  tpm: 100000
claude-3-haiku-20240307: # https://docs.anthropic.com/claude/reference/input-and-output-sizes
  max_tokens: 200000
  max_gen_tokens: 4096
  prompt_cost: 0.00025
  completion_cost: 0.00125
  rpm: 1
  tpm: 100000
claude-3-sonnet-20240229: # https://docs.anthropic.com/claude/reference/input-and-output-sizes
  max_tokens: 200000
  max_gen_tokens: 4096
  prompt_cost: 0.003
  completion_cost: 0.015
  rpm: 1
  tpm: 100000
claude-3-opus-20240229: # https://docs.anthropic.com/claude/reference/input-and-output-sizes
  max_tokens: 200000
  max_gen_tokens: 4096
  prompt_cost: 0.015
  completion_cost: 0.075
  rpm: 1
  tpm: 100000
claude-3-5-sonnet-20240620: # https://docs.anthropic.com/claude/reference/input-and-output-sizes
  max_tokens: 200000
  max_gen_tokens: 4096
  prompt_cost: 0.015
  completion_cost: 0.075
  rpm: 1
  tpm: 100000
llama-2-7b-chat:
  max_tokens: 4096
  prompt_cost: 0
  completion_cost: 0
  rpm: 0
  tpm: 0
# GOOGLE
# Two annoying TODOs for Google models
# 1. unlike other models, where max_tokens = prompt_tokens + completion_tokens, google has a max_output_tokens
# 2. unlike other models, pricing for Google models is per-character, not per-token -> 1 token ~ 4 char
# see: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-chat
#      https://cloud.google.com/vertex-ai/pricing#generative_ai_models
chat-bison:
  max_tokens: 4096
  max_gen_tokens: 1024
  prompt_cost: 0.0005
  completion_cost: 0.0005
  rpm: 60
  tpm: 240000  # 60 * 4000 -> likely lower in reality
chat-bison-32k:
  max_tokens: 32000
  max_gen_tokens: 8192
  prompt_cost: 0.0005
  completion_cost: 0.0005
  rpm: 60
  tpm: 600000  # 60 * 10000 -> likely lower in reality
# https://ai.google.dev/models/gemini
gemini-1.0-ultra:
  max_tokens: 30720
  max_gen_tokens: 2048
  rpm: 60
  tpm: 60
gemini-1.0-pro:
  max_tokens: 30720
  max_gen_tokens: 2048
  rpm: 60
  tpm: 60
  prompt_cost: 0.0005
  completion_cost: 0.0015
# currently 2x minute, 1000 queries per day max
gemini-1.5-pro-001:
  max_tokens: 1048576
  max_gen_tokens: 8192
  rpm: 1
  tpm: 60
  prompt_cost: 0.0
  completion_cost: 0.0
# COHERE
# https://cohere.com/pricing
command:
  max_tokens: 4096
  prompt_cost: 0.0003
  completion_cost: 0.0006
  rpm: 10000
  tpm: 0
command-r-plus:
  max_tokens: 4096
  prompt_cost: 0.0003
  completion_cost: 0.0006
  rpm: 10000
  tpm: 0
command-light:
  max_tokens: 4096
  prompt_cost: 0.0003
  completion_cost: 0.0006
  rpm: 10
  tpm: 0 
# TOGETHER.AI models 
# https://docs.together.ai/docs/inference-models
# pricing: https://www.together.ai/pricing
google/gemma-2b-it: 
  max_tokens: 8192
  prompt_cost: 0.0001
  completion_cost: 0.0001
  rpm: 0
  tpm: 0
google/gemma-7b-it:
  max_tokens: 8192
  prompt_cost: 0.0002
  completion_cost: 0.0002
  rpm: 0
  tpm: 0
meta-llama/Llama-2-70b-chat-hf:
  max_tokens: 4096
  prompt_cost: 0.0009
  completion_cost: 0.0009
  rpm: 0
  tpm: 0
meta-llama/Llama-2-13b-chat-hf:
  max_tokens: 4096
  prompt_cost: 0.00025
  completion_cost: 0.00025
  rpm: 0
  tpm: 0
meta-llama/Llama-2-7b-chat-hf:
  max_tokens: 4096
  prompt_cost: 0.0002
  completion_cost: 0.0002
  rpm: 0
  tpm: 0
meta-llama/Llama-3-70b-chat-hf:
  max_tokens: 4096
  prompt_cost: 0.0009
  completion_cost: 0.0009
  rpm: 0
  tpm: 0
meta-llama/Meta-Llama-3-70B:
  max_tokens: 4096
  prompt_cost: 0.0009
  completion_cost: 0.0009
  rpm: 0
  tpm: 0
meta-llama/Llama-3-8b-chat-hf:
  max_tokens: 4096
  prompt_cost: 0.0009
  completion_cost: 0.0009
  rpm: 0
  tpm: 0
meta-llama/Meta-Llama-3-8B:
  max_tokens: 4096
  prompt_cost: 0.0009
  completion_cost: 0.0009
  rpm: 0
  tpm: 0
mistralai/Mistral-7B-Instruct-v0.1:
  max_tokens: 8192
  prompt_cost: 0.0002
  completion_cost: 0.0002 
  rpm: 0
  tpm: 0
mistralai/Mistral-7B-Instruct-v0.2:
  max_tokens: 32768
  prompt_cost: 0.0002
  completion_cost: 0.0002
  rpm: 0
  tpm: 0
mistralai/Mixtral-8x7B-Instruct-v0.1:
  max_tokens: 32768
  prompt_cost: 0.0006
  completion_cost: 0.0006
  rpm: 0
  tpm: 0
mistralai/Mixtral-8x22B-Instruct-v0.1:
  max_tokens: 32768
  prompt_cost: 0.0006
  completion_cost: 0.0006
  rpm: 0
  tpm: 0